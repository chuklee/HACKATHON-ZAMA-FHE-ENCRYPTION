{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilyas/miniconda3/envs/ppai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from concrete.compiler import parameter\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from concrete.ml import deployment\n",
    "import numpy as np\n",
    "from concrete import fhe\n",
    "from concrete.ml.deployment import FHEModelClient, FHEModelDev, FHEModelServer\n",
    "from concrete.ml.sklearn import SGDClassifier\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9893617021276596\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from preprocess import load_dataset, featurisation\n",
    "from sklearn.model_selection import train_test_split\n",
    "embeddings, labels = load_dataset(\n",
    "    \"./data/lfw_people/George_HW_Bush\", cache=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(C=1/5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute difference in predictions: 0.0010453411443437591\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 1: Train a simple sklearn model (e.g., Logistic Regression)\n",
    "sklearn_model = LogisticRegression()\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "class PyTorchLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PyTorchLogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return (self.linear(x) > 0).float()\n",
    "\n",
    "# Step 3: Extract parameters from sklearn model\n",
    "sklearn_weights = sklearn_model.coef_[0]\n",
    "sklearn_bias = sklearn_model.intercept_[0]\n",
    "\n",
    "# Step 4: Initialize and load parameters into PyTorch model\n",
    "pytorch_model = PyTorchLogisticRegression(input_dim=128)\n",
    "pytorch_model.linear.weight.data = torch.FloatTensor(sklearn_weights).unsqueeze(0)\n",
    "pytorch_model.linear.bias.data = torch.FloatTensor([sklearn_bias])\n",
    "\n",
    "sklearn_pred = sklearn_model.predict_proba(X_train)[:, 1]\n",
    "pytorch_pred = pytorch_model(torch.FloatTensor(X_train)).detach().numpy().flatten()\n",
    "\n",
    "print(\"Mean absolute difference in predictions:\", np.mean(np.abs(sklearn_pred - pytorch_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((375, 128), (128,), ())"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train.shape, sklearn_weights.shape, sklearn_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concrete.ml.torch.compile import compile_torch_model\n",
    "import numpy\n",
    "\n",
    "N_FEAT = 3\n",
    "# Convert numpy arrays to torch tensors if they aren't already\n",
    "torch_input = torch.from_numpy(X_train) if isinstance(X_train, np.ndarray) else X_train\n",
    "\n",
    "\n",
    "quantized_module = compile_torch_model(\n",
    "    pytorch_model, # our model\n",
    "    torch_input, # a representative input-set to be used for both quantization and compilation\n",
    "    n_bits=6,\n",
    "    rounding_threshold_bits={\"n_bits\": 6, \"method\": \"approximate\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = quantized_module.forward(X_test, fhe=\"execute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.ravel() - y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
